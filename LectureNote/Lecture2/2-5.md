## 2-5. 웨이트 초기화 기법들  
## # > Initial weight
TYPE1) **LeCun**  
TYPE2) **Kaiming**  
TYPE1) **LeCun**  

|공통점|랜덤하게 0 근처로 잡자|
|--|--|
|**차이점**|**분산이 다름**|

<img src="https://github.com/user-attachments/assets/426b995a-a073-4ea9-9bce-e9dbc03a53f4" alt="description" style="width:70%; height:auto;">  
  
> TYPE1) **LeCun**  
- w : wait  
- U : Uniform, 균등 분포  
- N : Normal, 정규 분포 (평균과 분산의 함수)  
- N_in : 입력 개수  
- N_out : 출력 개수
  
> TYPE2) **Kaiming**  

LeCun함수에서 분산 2배
  
> TYPE3) **Xavier** 

분산이 더 작아짐
  
**Q.** N_in과 N_out를 사용하는 경우?  
**A.** N_in이 매우 많을 경우, 분산이 더 커질 것임.  
&nbsp;&nbsp;&nbsp;&nbsp;이러면 activation으로 출력할 때 불안정해짐
  
Q & A  
Q. ReLU가 무엇인가?
A. 신경망에서 자주 사용되는 활성화 함수로, 입력이 양수면 그대로 출력, 음수면 0으로 출력  

Q. sigmoid/tanh이 무엇인가?  
A. sigmoid: 출력값이 0과 1 사이, 확률 값처럼 사용 가능  
Tahn: 출력값이 -1과 1 사이  
  
Q. 분산의 정의가 구체적으로 무엇인가?  
A.  데이터의 평균오부터 편차(차이)의 제곱을 평균낸 값  

---
