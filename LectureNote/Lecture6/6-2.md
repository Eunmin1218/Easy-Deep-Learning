## 6-2. 로지스틱 회귀  
### sigmoid를 이용한 이진 분류  
- 총 parameter 개수 : gardient의 길이  
- **`Loss`** : 인공신경망의 출력의 의미를 정함
  #### →출력이 잘 나오도록 `Loss`를 잘 정하는 것이 중요하다!!

> ### BCE (Binary Cross-Entropy)
: 이진 분류에 적합한 Loss 함수 구하는 법  
  
예를 들어,  강아지 사진일 경우 **`q`** 를 1에 가깝게 **최대화**,  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;고양이 사진일 경우 **`1-q`** 를 **최대화**  
  
만약 batch-size가 2 이상일 경우  

$$
P(y_1 ∩ y_2) = P(y_1)P(y_2)
$$

그러나 곱하면 계속 작아지므로 

$$
BCE에서 Loss 값 = -\frac{1}{k} \sum_{n=1}^{k} \log ( {q}_n^{y_n}  ({1-q}_n)^{1-y_n} )
$$

<img src="https://github.com/user-attachments/assets/b36ee7aa-d8d7-4e77-81fd-88223378780e" style="width:80%; height:auto;">

즉 **분류 ⊂ 회귀**
> ### Logistic Regression
: 입력과 출력 사이의 관계를 확률 함수로 표현  
  
**`logit`**: *log*(**odds**)  
**`odds`**: 승산, *q* / *1-q*  
위에 의해 &nbsp;&nbsp; ***l*** = ***1*** / ***1*** + ***e⁻ˡ***  
즉, logit에 sigmoid 함수를 적용하면 확률값을 얻을 수 있다.  

로지스틱 회귀는  
1. **logit를 linear regression으로 구한 것으로도 해석 가능**  
2. **즉, sigmoid함수를 통과시켜 logit를 확률을 구했다!**

> ### 결론    
> 인공 신경망의 역할이 입력 사진과 Logit 사이의 선형 관계를 찾는 것이며,  
> sigmoid는 이 Logit을 확률로 벼환하고 Loss를 계산하기 위해 사용되는 함수

---
