## 6-5. 소프트맥스 회귀  
> ### softmax 회귀 ( Multinomial Logistic Regression )  
- 출력 노드의 수 늘리기
- Loss 함수 적절히 지정
- 마지막 activation은 **`softmax`**  
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;↳ **`softmax`** : 출력값의 부호를 구분하여 이를 반영 가능하기에 사용
  <img src="https://github.com/user-attachments/assets/e74c5ea3-2063-4ca6-98d5-0d8cf8889a71" style="width:50%; height:auto;">

여기서도 MLE로 생각  
**multinoulli 분포**를 따른다고 가정  
-> **`cross-entropy loss`**  
>  ### Cross-Entropy Loss
: 베르누이 분포를 확장했다고 보면 됨  
<img src="https://github.com/user-attachments/assets/3a745bcf-32c6-47b3-b05a-a9c717e5d8ca" style="width:75%; height:auto;">  

**1st.** 베르누이 분포에서와 비슷하게 **Likelihood** 를 구하기  
**2nd.** 실제 분포와 예측 분포의 **cross-entropy** 구하기  
**3rd.** **`실제 분포의 엔트로피 <= cross 엔트로피`** 라는 점 이용해서 weight 계산  

이 때, 해당 클래스를 담당하는 노드 이외의 출력값은 고려하지 않아도 됨!  
  
Q&A
Q. 엔트로피?  
A. 불확실성. 엔트로피가 높다는 건 정보가 많고 확률이 낮다는 것  

---
