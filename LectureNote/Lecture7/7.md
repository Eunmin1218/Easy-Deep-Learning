# Thm7. 인공 신경망, 그 한계는 어디까지인가?  
## 7. Universal Approximation Theorem (보편 근사 정리)  
왜 굳이 이런 형태로 함수를 표현해야 하는가?  
이런 형태면 히든 레이어 한 줄만 있으면 어떤 연속 함수든 나타낼 수 있기 때문이다!  
**hidden layer** : non-linear acivation  
**output layer** : linear activation  
> ### Universal Approximation Theorem  
#### 즉, 어떤 연속 함수든 `f(xW₁ + b₁)W₂ + b₂`로 표현 가능하다  
&nbsp;&nbsp;&nbsp;&nbsp;-> `train loss를 딱 0으로 만들 수 있다`  

<img src="https://github.com/user-attachments/assets/2d1ff708-705e-44c0-ae8e-2905453f7476" style="width:50%; height:auto;">
  
그럼 왜 DNN을 사용할까? 
1. **효율**을 위해
2. 그저 이렇게 모든 표현이 가능하다는 것 뿐!
   학습에서는 이렇게 하지 않는 것이 좋다.

하지만 무조건 깊게만 만들어도..  
1. vanishing gradient 문제
   **`vanishing gradient`**: layer가 많으면 입력 층에 가까울수록 미분이 사라진다  
2. overfitting 문제
3. loss landscape가 꼬불꼬불해지는 문제  

---
