## 8-1. Vanishing Gradient(기울기 소실)와 ReLU    
> #### Vanishing gradient
: `layer가 많으면 입력 층에 가까울수록 미분이 사라진다` (식당 비유 생각)  
&nbsp;역전파 부분을 참고했을 때, **sigmoid 함수의 최대 기울기가 너무 작아 발생**  

오히려 **underfitting** 문제가 발생한다  
따라서, `노드 수가 매우 많을 때` ReLU 사용  

> ### ReLU (Reflected Linear Unit)
: 음수는 0 출력, 양수는 들어온 대로 출력  

-> **`적어도 activation 미분에 의해 값이 작아지지는 않는다`**  

- > ##### Leaky ReLU
  : 음수쪽에도 기울기 형성(y = 0.01x)  
- > ##### Parameter ReLU
  : 음수쪽에서 y = ax로 두고 a에 대한 편미분 구하기  
  
그런데 만약 모든 지점에서 y = x형태라면?  
-> **`not linear`** 을 확보하기 위해서이다  

즉, **sigmoid**를 사용할 경우 티끌 모아 티끌  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**ReLU**는 살릴 것만 살리자  

---
