---
## 3-3. mini-batch GD  
SGD는 하나씩만 보니까 너무 성급하게 방향 결정  
sol) **`mini-batch`**
> ### mini-batch
: 하나가 아니라 여러개를 보는 것  
GPU는 병렬 연산을 가능하게 하기 때문 -> GD의 속도 단점 해결  
  
그렇다면 왜 mini-batch를 사용하는가?  
**1st.** learning rate를 batch size와 비례하여 키우기  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  이렇게 하면 BS가 더 작을 때와 비슷한 결과를 얻을 수 있게 됨  
**2nd.** warmup 하기  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **warmup** : 처음에 Learning Rate를 0에서부터 출력하여 올려주는 것  
  
위의 두 가지를 하면 작은 batch size일 때 8K까지는 그래도 error 줄어들음  

> Hyperparameter (내가 직접 설정해야 하는 변수)  
```yaml
총 Epoch 수: 전체 데이터를 몇 번 반복해서 풀거냐  
Batch size: 몇 개씩 볼거냐  
Learning rate: 얼만큼 갈건가  
```
|prarameter|hyperparameter|
|:--:|:--:|
|AI가 스스로 알아내는 변수|내가 정해줘야 하는 변수|
|예) weight, bias 등|예) Epoch, batch size, Initial weight, learning rate & learning rate scheduling 등|

▼ ` 그 외 hyper parameter `
- model architecture(layer 수, node 수, activation 함수 등..)
- loss 함수 뭐 쓸지
- 최적화 기법 뭐 쓸지


---
