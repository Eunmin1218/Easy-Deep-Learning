## 3-2. 확률적 경사 하강법(SGD: Stochastic Gradient Descent)  
GD의 두 가지 문제점을 완화

### " 랜덤하게 데이터를 하나씩 뽑아서 loss를 만든다 "  
```yaml 
1st: 데이터 중 하나를 뽑아 loss를 만든다.    
2nd: 그 값을 L로 이용하여 Gradient를 업데이트한 후 한 번 계산한다.  
3rd: 나머지 데이터 중 하나를 뽑아 loss를 만들고 2nd를 실행한다.  
4th: 데이터가 없을 경우 다시 채워 위를 반복한다.  
5th: R값이 충분히 수렴할 때까지 이를 반복한다.  
```
**Q.** 왜 gradient가 GD에서의 minimum을 향하지 않을까?  
**A.** 데이터 중 하나를 뽑아 만든 L 그래프이기 때문이다. 

> 1. 하나만 보고 빠르게 방향을 결정  -> 첫 번째 문제 해결
> 2. 확률적으로 더 나은 minimin을 찾을 수 있음 -> 두 번째 문제 해결

---
