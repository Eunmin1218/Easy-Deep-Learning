## 3-5. RMSProp  
|Momontum|RMSProp|
|--|--|
|Gradient 누적|Gradient **크기만** 누적|

 > ### RMSProp (Root Mean Squared Propagation)  
: Learning rate를 각 파라미터 별로 다르게 줌  
  
왜 이 방식을 사용하는가?    
만약 Gradient에서 하나가 매우 클 경우, 불균형한 상황이 발생한다.  
따라서 이러한 상황을 해결하기 위해 Learning rate를 다르게 줌으로써 평준화를 하는 것이다.  

그냥 Gradient를 사용할 경우, Local minimum이 발생할 가능성이 높고, 가파른 곳에 갇힐 가능성이 있다.   
그러나 RMSProp을 사용할 경우,  
`가파른 쪽에서는 조심조심 가고, 완만한 쪽에서는 과감하게 감으로써`  이상한 곳에 빠지는 걸 방지할 수 있다.  
왜 이렇게 가는 것인가?  
1. 가파른 곳에서는 작은 이동으로도 Loss값이 크게 변함. 따라서 많이 이동할 경우 최저점을 지나칠 수 있음.  
2. 완만한 부분은 많이 이동해도 Loss값이 크게 변하지 않음. 따라서 과감하게 이동하여 평평한 곳을 빠르게 탈출함.  

Momentum과 RMSProp을 합친 것이 **Adam**

---
