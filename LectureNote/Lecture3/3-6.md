## 3-6. Adam(Adaptive Moment Estimation)  
<img src="https://github.com/user-attachments/assets/e0e49405-9fd5-474b-b6e8-143ad2a73baa" alt="description" style="width:50%; height:auto;">  

> ### Adam
: momentum과 RMSProp를 같이 반영한 최적화 알고리즘  

Adam식에서..  
1. **분자는 momentum** 반영: gradiemt를 누적  
2. **분모는 RMSProp** 반영: gradient의 크기 누적해 루트 처리  

|momentum|RMSProp|
|:--:|:--:|
|방향|보폭|
  
> #### 편향 보정
: Adam 수식에서 vₖ와 mₖ를 성분별로 나눔  
왜 사용하는가?: m0과 v0의 초깃값이 0이라서 학습률이 작아질 수 있음  


  
+SGD는 안장점애서 탈출을 잘 못함.  
왜? SGD는 현재 시점의 미분만 생각하기 때문.  
 
  
Q & A  
Q. element wise가 무엇인가?  
A. 행렬(또는 벡터) 연산에서, 각 원소(element)끼리 독립적으로 연산이 수행되는 방식  

Q. ϵ는 무엇인가?  
A. 수치 안정성을 위해 더해지는 작은 값이다.  

---
# Thm.4  
## 4-1. Validation 데이터  
> **Traning** vs **Test** vs **Validation**

`Training` : 정답 알려주며 훈련  
`Test` : 처음보는 데이터로 테스트  

그런데 Training data만 가지고는 몇 **epoch**에서 학습을 멈춰야 할 지 모름  
만약 Epoch가 너무 클 경우 (학습을 너무 많이 시켰을 경우), 
오버피팅 현상이 나타나게 됨 ( Test Loss가 너무 커짐 )  

그러나 그렇다고 Test Loss에 맞추어 Epoch를 정하면 안됨.  
Test data는 훈련 전 과정에 관여해서는 안되기 때문.    
그래서 **`validation data`** 가 필요함.  

> ### Validation Data
: Train data의 일부를 따로 떼 내어 test data처럼 사용 ( grad 구하는 데에 참여하지 않은 데이터 )  

|Training data|Test data|Validation data|
|:--:|:--:|:--:|
|파라미터 학습을 위한 data|최종적으로 학습된 모델 테스트용 data|하이퍼파라미터 선택을 위한 data|  

Val data를 보고..  
1. val loss가 가장 작을 때까지만 학습한다. (총 Epoch 수)
2. val loss를 작게 만드는 모델을 고른다. (model 구조)

---
