## 8-2. Batch Normalization(배치 정규화) & Layer Normalization(레이어 정규화)    
<img src="https://github.com/user-attachments/assets/7585922b-6619-4ffe-9135-96cefba93e03" style="width:30%; height:auto;">  

만약 위와 같은 경우, `notlinear activation`을 살리지 못함  
  
> ### Batch Normalization
: 입력 값들을 적절하게 **재배치** 시키는 것 ( 순서 유지 )  
&nbsp;&nbsp;**`평균과 분산을 학습시키자!`**

  
Q&A  
Q. 엔트로피?  
A. 불확실성. 엔트로피가 높다는 건 정보가 많고 확률이 낮다는 것  
